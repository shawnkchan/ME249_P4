{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951ea808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Function that takes in two dataframe columns and plots a log-log graph\n",
    "Follows standardised formatting\n",
    "@input recorded: dataframe column of recorded data\n",
    "@input predicted: dataframe column of the predicted value\n",
    "@input xmin: lower bound of graph's x axis\n",
    "@input xmax: upper bound of graph's x axis\n",
    "@input ymin: lower bound of graph's y axis\n",
    "@input ymax: upper bound of graph's y axis\n",
    "@input xTitle: title for x axis\n",
    "@input yTitle: title for y axis\n",
    "@output log log graph of the preducted values against recorded values\n",
    "'''\n",
    "\n",
    "def logLogPlotter(recorded, predicted, xmin, xmax, ymin, ymax, xTitle, yTitle, unit):\n",
    "    x = recorded\n",
    "    y = predicted\n",
    "\n",
    "    coefficients = np.polyfit(np.log(x), np.log(y), 1)\n",
    "    line_function = np.poly1d(coefficients)\n",
    "\n",
    "    # Create a range for predicted values for the fit line\n",
    "    x_vals = np.linspace(min(x), max(y), 400)\n",
    "    y_vals = np.exp(line_function(np.log(x_vals)))\n",
    "\n",
    "    #calculate similarity measures\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "    #best fit line and y = x line\n",
    "    plt.scatter(x, y, c='red', marker='x')\n",
    "    plt.plot(x_vals, y_vals, color='red', label=f'Fit: y = {coefficients[0]:.2f}x + {coefficients[1]:.2f}')\n",
    "    plt.axline((0, 0), (1,1),color='black',linestyle='--', label=f'y=x, R^2 = {r_value ** 2}',)\n",
    "\n",
    "    #visuals\n",
    "    font_family = 'Times New Roman'\n",
    "    title_font_size=17\n",
    "    label_font_size=15\n",
    "    legend_tick_font_size=12\n",
    "\n",
    "    plt.title(f'Predicted {yTitle} against Recorded {xTitle}', fontsize=title_font_size, fontfamily=font_family)\n",
    "    plt.xlabel(f'Recorded {xTitle} ({unit})', fontsize=label_font_size, fontfamily=font_family)\n",
    "    plt.ylabel(f'Predicted {yTitle} ({unit})', fontsize=label_font_size, fontfamily=font_family)\n",
    "\n",
    "    plt.legend(fontsize = legend_tick_font_size)\n",
    "    plt.loglog()\n",
    "    plt.xlim(xmax=xmax, xmin=xmin)\n",
    "    plt.ylim(ymax=ymax, ymin=ymin)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978eb101",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''>>>>> start CodeP4.1F23\n",
    "V.P. Carey ME249, Fall 2023\n",
    "Keras Neural Network Modeling '''\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the following 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# define meadian values of input variables - add your values here\n",
    "\n",
    "import statistics\n",
    "import numpy\n",
    "xdata = []\n",
    "ydata = []\n",
    "#xdata.append([ Di(m), qoflux (kW/m^2), exit quality, max wall temperature (deg C)])\n",
    "xdata.append([0.008, 550, 0.525, 306.7])\n",
    "xdata.append([0.008, 650, 0.525, 298.5])\n",
    "xdata.append([0.008, 750, 0.525, 294.2])\n",
    "xdata.append([0.008, 850, 0.525, 290.2])\n",
    "xdata.append([0.008, 950, 0.524, 286.9])\n",
    "xdata.append([0.008, 1050, 0.524, 284.1])\n",
    "xdata.append([0.008, 1150, 0.525, 281.7])\n",
    "xdata.append([0.008, 850, 0.524, 290.31])\n",
    "xdata.append([0.008, 550, 0.734, 307.9])\n",
    "xdata.append([0.008, 750, 0.735, 295.5])\n",
    "xdata.append([0.008, 950, 0.735, 287.8])\n",
    "xdata.append([0.008, 1050, 0.735, 285.0])\n",
    "xdata.append([0.008, 1150, 0.735, 282.5])\n",
    "xdata.append([0.008, 850, 0.734, 291.3])\n",
    "xdata.append([0.008, 550, 0.945, 308.6])\n",
    "xdata.append([0.008, 750, 0.945, 296.2])\n",
    "xdata.append([0.008, 950, 0.945, 288.5])\n",
    "xdata.append([0.008, 1150, 0.945, 283.1])\n",
    "xdata.append([0.008, 850, 0.945, 291.9])\n",
    "xdata.append([0.011, 550, 0.525, 328.0])\n",
    "xdata.append([0.011, 750, 0.525, 311.2])\n",
    "xdata.append([0.011, 950, 0.525, 300.8])\n",
    "xdata.append([0.011, 1150, 0.525, 293.6])\n",
    "xdata.append([0.011, 850, 0.525, 305.5])\n",
    "xdata.append([0.011, 550, 0.735, 329.6])\n",
    "xdata.append([0.011, 750, 0.735, 312.6])\n",
    "xdata.append([0.011, 950, 0.735, 302.0])\n",
    "xdata.append([0.011, 1050, 0.735, 299.4])\n",
    "xdata.append([0.011, 1150, 0.735, 294.8])\n",
    "xdata.append([0.011, 850, 0.735, 306.8])\n",
    "xdata.append([0.011, 550, 0.945, 330.7])\n",
    "xdata.append([0.011, 750, 0.945, 313.6])\n",
    "xdata.append([0.011, 950, 0.944, 302.9])\n",
    "xdata.append([0.011, 1150, 0.945, 295.6])\n",
    "xdata.append([0.011, 850, 0.944, 307.7])\n",
    "xdata.append([0.011, 700, 0.734, 324.7])\n",
    "xdata.append([0.013, 550, 0.525, 342.2])\n",
    "xdata.append([0.013, 750, 0.524, 322.3])\n",
    "xdata.append([0.013, 950, 0.524, 310.0])\n",
    "xdata.append([0.013, 1150, 0.525, 301.6])\n",
    "xdata.append([0.013, 850, 0.524, 315.5])\n",
    "xdata.append([0.013, 550, 0.734, 344.1])\n",
    "xdata.append([0.013, 750, 0.735, 324.0])\n",
    "xdata.append([0.013, 950, 0.735, 311.5])\n",
    "xdata.append([0.013, 1050, 0.735, 306.3])\n",
    "xdata.append([0.013, 1150, 0.735, 302.9])\n",
    "xdata.append([0.013, 850, 0.734, 317.1])\n",
    "xdata.append([0.013, 550, 0.945, 345.3])\n",
    "xdata.append([0.013, 750, 0.944, 325.1])\n",
    "xdata.append([0.013, 950, 0.944, 312.5])\n",
    "xdata.append([0.013, 1150, 0.945, 303.9])\n",
    "xdata.append([0.013, 850, 0.945, 318.2])\n",
    "\n",
    "#data.append([mdot (kg/s)])\n",
    "ydata.append([0.06157])\n",
    "ydata.append([0.07269])\n",
    "ydata.append([0.08396])\n",
    "ydata.append([0.09347])\n",
    "ydata.append([0.10635])\n",
    "ydata.append([0.11521])\n",
    "ydata.append([0.1287])\n",
    "ydata.append([0.09516])\n",
    "ydata.append([0.04398])\n",
    "ydata.append([0.05997])\n",
    "ydata.append([0.07596])\n",
    "ydata.append([0.08343])\n",
    "ydata.append([0.0919])\n",
    "ydata.append([0.06797])\n",
    "ydata.append([0.0342])\n",
    "ydata.append([0.04664])\n",
    "ydata.append([0.05908])\n",
    "ydata.append([0.0715])\n",
    "ydata.append([0.05286])\n",
    "ydata.append([0.0846])\n",
    "ydata.append([0.1154])\n",
    "ydata.append([0.1462])\n",
    "ydata.append([0.177])\n",
    "ydata.append([0.1308])\n",
    "ydata.append([0.06047])\n",
    "ydata.append([0.08246])\n",
    "ydata.append([0.1044])\n",
    "ydata.append([0.1134])\n",
    "ydata.append([0.1264])\n",
    "ydata.append([0.0934])\n",
    "ydata.append([0.047])\n",
    "ydata.append([0.06413])\n",
    "ydata.append([0.08124])\n",
    "ydata.append([0.09834])\n",
    "ydata.append([0.072691])\n",
    "ydata.append([0.087196])\n",
    "ydata.append([0.10005])\n",
    "ydata.append([0.13644])\n",
    "ydata.append([0.17282])\n",
    "ydata.append([0.2092])\n",
    "ydata.append([0.15463])\n",
    "ydata.append([0.07147])\n",
    "ydata.append([0.09745])\n",
    "ydata.append([0.12344])\n",
    "ydata.append([0.13302])\n",
    "ydata.append([0.1494])\n",
    "ydata.append([0.11045])\n",
    "ydata.append([0.05558])\n",
    "ydata.append([0.0758])\n",
    "ydata.append([0.09601])\n",
    "ydata.append([0.1162])\n",
    "ydata.append([0.0859])\n",
    "xarray= numpy.array(xdata)\n",
    "yarray= numpy.array(ydata)\n",
    "x_original = np.array(xdata)\n",
    "y_original = np.array(ydata)\n",
    "# print (xdata)\n",
    "# print (xarray)\n",
    "# print (ydata)\n",
    "# print (yarray)\n",
    "\n",
    "#convert to:\n",
    "# median values of output variables\n",
    "ND=len(xarray)\n",
    "Diall = [row[0] for row in xdata]\n",
    "Dimed=statistics.median(Diall)\n",
    "q0all = [row[1] for row in xdata]\n",
    "q0med=statistics.median(q0all)\n",
    "xeall = [row[2] for row in xdata]\n",
    "xemed=statistics.median(xeall)\n",
    "Twall = [row[3] for row in xdata]\n",
    "Twmed=statistics.median(Twall)\n",
    "\n",
    "mall = [row[0] for row in ydata]\n",
    "mmed=statistics.median(mall)\n",
    "\n",
    "xdata = []\n",
    "ydata = []\n",
    "for i in range(ND):\n",
    "    xdata.append([xarray[i,0]/Dimed,xarray[i,1]/q0med,xarray[i,2]/xemed,xarray[i,3]/Twmed])\n",
    "    ydata.append([yarray[i,0]/mmed])\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "yarray= np.array(ydata)\n",
    "\n",
    "#print (yarray)\n",
    "\n",
    "data_inputs = np.array(xdata)\n",
    "data_outputs = np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9abf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata_df = pd.DataFrame(x_original, columns=['Di', 'qoflux', 'xe', 'mdot'])\n",
    "ydata_df = pd.DataFrame(y_original, columns=['mdot'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata_normalized = xdata_df.divide(xdata_df.median())\n",
    "ydata_normalized = ydata_df.divide(ydata_df.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379a75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_input,validation_input,training_output,validation_output = train_test_split(xarray,\n",
    "yarray, test_size=0.25, random_state=13)\n",
    "# print to check the shape of training and validation set\n",
    "training_input= np.array(training_input)#[:,[1,2,3]])\n",
    "training_output= np.array(training_output)\n",
    "validation_input= np.array(validation_input)#[:,[1,2,3]])\n",
    "validation_output= np.array(validation_output)\n",
    "\n",
    "print(validation_input)\n",
    "\n",
    "xarray=training_input\n",
    "yarray=training_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dc6a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.DataFrame(training_input, columns=['Di', 'qoflux', 'xe', 'mdot'])\n",
    "x_validate = pd.DataFrame(validation_input,columns=['Di', 'qoflux', 'xe', 'mdot'])\n",
    "\n",
    "y_train = pd.DataFrame(training_output, columns=['mdot'])\n",
    "y_validate = pd.DataFrame(validation_output, columns=['mdot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "#As seen below, we have created four dense layers.\n",
    "#A dense layer is a layer in neural network that’s fully connected.\n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case.\n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 0.5\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "model = keras.Sequential([\n",
    "keras.layers.Dense(6, activation=K.elu, input_shape=[4], kernel_initializer=initializer),\n",
    "keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(12, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(16, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(2, kernel_initializer=initializer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation.\n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks.\n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here\n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.01)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a87d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training.\n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again.\n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs\n",
    "#I found acceptable prediction accuracy.\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs.\n",
    "#During model training, if all the batches of data are seen by the model once,\n",
    "#we say that one epoch has been completed.\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "monitor='loss',\n",
    "mode='min',\n",
    "patience = 90,\n",
    "restore_best_weights = True,\n",
    "verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss',\n",
    "mode='min', verbose=1, save_best_only=True)\n",
    "historyData = model.fit(xarray,yarray,epochs=800,callbacks=[es])\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n",
    "model.save('./best_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137a44d",
   "metadata": {},
   "source": [
    "## (e) Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb462f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions_df = pd.DataFrame(model.predict(x_train), columns=['mdot'])\n",
    "y_predictions_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08ec1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_recorded_df = y_train\n",
    "y_recorded_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d43d7a",
   "metadata": {},
   "source": [
    "### MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7cb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(y_recorded_df['mdot'], y_predictions_df['mdot'])\n",
    "print(f'MAE: {mae}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b4bcb6",
   "metadata": {},
   "source": [
    "### Log-log Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logLogPlotter(y_recorded_df['mdot'], y_predictions_df['mdot'], -0.5, 1.5, -0.5, 1.5, 'mdot', 'mdot', '')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
