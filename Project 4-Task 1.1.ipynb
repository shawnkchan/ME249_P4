{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "978eb101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.008, 550, 0.06157], [0.008, 651, 0.07269], [0.008, 750, 0.08396], [0.008, 850, 0.09347], [0.008, 950, 0.10635], [0.008, 1050, 0.11521], [0.008, 1150, 0.1287], [0.008, 850, 0.09516], [0.008, 550, 0.04398], [0.008, 750, 0.05997], [0.008, 950, 0.07596], [0.008, 1050, 0.08343], [0.008, 1150, 0.0919], [0.008, 850, 0.06797], [0.008, 550, 0.0342], [0.008, 750, 0.04664], [0.008, 950, 0.05908], [0.008, 1150, 0.0715], [0.008, 850, 0.05286], [0.011, 550, 0.0846], [0.011, 750, 0.1154], [0.011, 950, 0.1462], [0.011, 1150, 0.177], [0.011, 850, 0.1308], [0.011, 550, 0.06047], [0.011, 750, 0.08246], [0.011, 950, 0.1044], [0.011, 1050, 0.1134], [0.011, 1150, 0.1264], [0.011, 850, 0.0934], [0.011, 550, 0.047], [0.011, 750, 0.06413], [0.011, 950, 0.08124], [0.011, 1150, 0.09834], [0.011, 850, 0.072691], [0.011, 700, 0.087196], [0.013, 550, 0.10005], [0.013, 750, 0.13644], [0.013, 950, 0.17282], [0.013, 1150, 0.2092], [0.013, 850, 0.15463], [0.013, 550, 0.07147], [0.013, 750, 0.09745], [0.013, 950, 0.12344], [0.013, 1050, 0.13302], [0.013, 1150, 0.1494], [0.013, 850, 0.11045], [0.013, 550, 0.05558], [0.013, 750, 0.0758], [0.013, 950, 0.09601], [0.013, 1150, 0.1162], [0.013, 850, 0.0859]]\n",
      "[[8.0000e-03 5.5000e+02 6.1570e-02]\n",
      " [8.0000e-03 6.5100e+02 7.2690e-02]\n",
      " [8.0000e-03 7.5000e+02 8.3960e-02]\n",
      " [8.0000e-03 8.5000e+02 9.3470e-02]\n",
      " [8.0000e-03 9.5000e+02 1.0635e-01]\n",
      " [8.0000e-03 1.0500e+03 1.1521e-01]\n",
      " [8.0000e-03 1.1500e+03 1.2870e-01]\n",
      " [8.0000e-03 8.5000e+02 9.5160e-02]\n",
      " [8.0000e-03 5.5000e+02 4.3980e-02]\n",
      " [8.0000e-03 7.5000e+02 5.9970e-02]\n",
      " [8.0000e-03 9.5000e+02 7.5960e-02]\n",
      " [8.0000e-03 1.0500e+03 8.3430e-02]\n",
      " [8.0000e-03 1.1500e+03 9.1900e-02]\n",
      " [8.0000e-03 8.5000e+02 6.7970e-02]\n",
      " [8.0000e-03 5.5000e+02 3.4200e-02]\n",
      " [8.0000e-03 7.5000e+02 4.6640e-02]\n",
      " [8.0000e-03 9.5000e+02 5.9080e-02]\n",
      " [8.0000e-03 1.1500e+03 7.1500e-02]\n",
      " [8.0000e-03 8.5000e+02 5.2860e-02]\n",
      " [1.1000e-02 5.5000e+02 8.4600e-02]\n",
      " [1.1000e-02 7.5000e+02 1.1540e-01]\n",
      " [1.1000e-02 9.5000e+02 1.4620e-01]\n",
      " [1.1000e-02 1.1500e+03 1.7700e-01]\n",
      " [1.1000e-02 8.5000e+02 1.3080e-01]\n",
      " [1.1000e-02 5.5000e+02 6.0470e-02]\n",
      " [1.1000e-02 7.5000e+02 8.2460e-02]\n",
      " [1.1000e-02 9.5000e+02 1.0440e-01]\n",
      " [1.1000e-02 1.0500e+03 1.1340e-01]\n",
      " [1.1000e-02 1.1500e+03 1.2640e-01]\n",
      " [1.1000e-02 8.5000e+02 9.3400e-02]\n",
      " [1.1000e-02 5.5000e+02 4.7000e-02]\n",
      " [1.1000e-02 7.5000e+02 6.4130e-02]\n",
      " [1.1000e-02 9.5000e+02 8.1240e-02]\n",
      " [1.1000e-02 1.1500e+03 9.8340e-02]\n",
      " [1.1000e-02 8.5000e+02 7.2691e-02]\n",
      " [1.1000e-02 7.0000e+02 8.7196e-02]\n",
      " [1.3000e-02 5.5000e+02 1.0005e-01]\n",
      " [1.3000e-02 7.5000e+02 1.3644e-01]\n",
      " [1.3000e-02 9.5000e+02 1.7282e-01]\n",
      " [1.3000e-02 1.1500e+03 2.0920e-01]\n",
      " [1.3000e-02 8.5000e+02 1.5463e-01]\n",
      " [1.3000e-02 5.5000e+02 7.1470e-02]\n",
      " [1.3000e-02 7.5000e+02 9.7450e-02]\n",
      " [1.3000e-02 9.5000e+02 1.2344e-01]\n",
      " [1.3000e-02 1.0500e+03 1.3302e-01]\n",
      " [1.3000e-02 1.1500e+03 1.4940e-01]\n",
      " [1.3000e-02 8.5000e+02 1.1045e-01]\n",
      " [1.3000e-02 5.5000e+02 5.5580e-02]\n",
      " [1.3000e-02 7.5000e+02 7.5800e-02]\n",
      " [1.3000e-02 9.5000e+02 9.6010e-02]\n",
      " [1.3000e-02 1.1500e+03 1.1620e-01]\n",
      " [1.3000e-02 8.5000e+02 8.5900e-02]]\n",
      "[[0.525, 306.7], [0.525, 298.5], [0.525, 294.5], [0.525, 290.2], [0.524, 286.9], [0.524, 284.1], [0.525, 281.7], [0.524, 290.3], [0.734, 307.9], [0.735, 295.5], [0.735, 287.8], [0.735, 285.0], [0.735, 282.5], [0.734, 291.3], [0.945, 308.6], [0.945, 296.2], [0.945, 288.5], [0.945, 283.1], [0.945, 291.9], [0.525, 328.0], [0.525, 311.2], [0.525, 300.8], [0.525, 293.6], [0.525, 305.5], [0.735, 329.6], [0.735, 312.6], [0.735, 302.0], [0.735, 299.4], [0.735, 294.8], [0.735, 306.8], [0.945, 330.7], [0.945, 313.6], [0.944, 302.9], [0.945, 295.6], [0.944, 307.7], [0.734, 324.7], [0.525, 342.2], [0.524, 322.3], [0.524, 310.0], [0.525, 301.6], [0.524, 315.5], [0.734, 344.1], [0.735, 324.0], [0.735, 311.5], [0.735, 306.3], [0.735, 302.9], [0.734, 317.1], [0.945, 345.3], [0.944, 325.1], [0.944, 312.5], [0.945, 303.9], [0.945, 318.2]]\n",
      "[[  0.525 306.7  ]\n",
      " [  0.525 298.5  ]\n",
      " [  0.525 294.5  ]\n",
      " [  0.525 290.2  ]\n",
      " [  0.524 286.9  ]\n",
      " [  0.524 284.1  ]\n",
      " [  0.525 281.7  ]\n",
      " [  0.524 290.3  ]\n",
      " [  0.734 307.9  ]\n",
      " [  0.735 295.5  ]\n",
      " [  0.735 287.8  ]\n",
      " [  0.735 285.   ]\n",
      " [  0.735 282.5  ]\n",
      " [  0.734 291.3  ]\n",
      " [  0.945 308.6  ]\n",
      " [  0.945 296.2  ]\n",
      " [  0.945 288.5  ]\n",
      " [  0.945 283.1  ]\n",
      " [  0.945 291.9  ]\n",
      " [  0.525 328.   ]\n",
      " [  0.525 311.2  ]\n",
      " [  0.525 300.8  ]\n",
      " [  0.525 293.6  ]\n",
      " [  0.525 305.5  ]\n",
      " [  0.735 329.6  ]\n",
      " [  0.735 312.6  ]\n",
      " [  0.735 302.   ]\n",
      " [  0.735 299.4  ]\n",
      " [  0.735 294.8  ]\n",
      " [  0.735 306.8  ]\n",
      " [  0.945 330.7  ]\n",
      " [  0.945 313.6  ]\n",
      " [  0.944 302.9  ]\n",
      " [  0.945 295.6  ]\n",
      " [  0.944 307.7  ]\n",
      " [  0.734 324.7  ]\n",
      " [  0.525 342.2  ]\n",
      " [  0.524 322.3  ]\n",
      " [  0.524 310.   ]\n",
      " [  0.525 301.6  ]\n",
      " [  0.524 315.5  ]\n",
      " [  0.734 344.1  ]\n",
      " [  0.735 324.   ]\n",
      " [  0.735 311.5  ]\n",
      " [  0.735 306.3  ]\n",
      " [  0.735 302.9  ]\n",
      " [  0.734 317.1  ]\n",
      " [  0.945 345.3  ]\n",
      " [  0.944 325.1  ]\n",
      " [  0.944 312.5  ]\n",
      " [  0.945 303.9  ]\n",
      " [  0.945 318.2  ]]\n"
     ]
    }
   ],
   "source": [
    "'''>>>>> start CodeP4.1F23\n",
    "V.P. Carey ME249, Fall 2023\n",
    "Keras Neural Network Modeling '''\n",
    "#import useful packages\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "import keras.backend as kb\n",
    "import tensorflow as tf\n",
    "#the following 2 lines are only needed for Mac OS machines\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# define meadian values of input variables - add your values here\n",
    "\n",
    "import statistics\n",
    "xdata = []\n",
    "ydata = []\n",
    "#xdata.append([ Di(m), qoflux (kW/m^2), mdot (kg/s)])\n",
    "\n",
    "xdata.append([0.008, 550, 0.06157])\n",
    "xdata.append([0.008, 651, 0.07269])\n",
    "xdata.append([0.008, 750, 0.08396])\n",
    "xdata.append([0.008, 850, 0.09347])\n",
    "xdata.append([0.008, 950, 0.10635])\n",
    "xdata.append([0.008, 1050, 0.11521])\n",
    "xdata.append([0.008, 1150, 0.1287])\n",
    "xdata.append([0.008, 850, 0.09516])\n",
    "xdata.append([0.008, 550, 0.04398])\n",
    "xdata.append([0.008, 750, 0.05997])\n",
    "xdata.append([0.008, 950, 0.07596])\n",
    "xdata.append([0.008, 1050, 0.08343])\n",
    "xdata.append([0.008, 1150, 0.0919])\n",
    "xdata.append([0.008, 850, 0.06797])\n",
    "xdata.append([0.008, 550, 0.0342])\n",
    "xdata.append([0.008, 750, 0.04664])\n",
    "xdata.append([0.008, 950, 0.05908])\n",
    "xdata.append([0.008, 1150, 0.0715])\n",
    "xdata.append([0.008, 850, 0.05286])\n",
    "xdata.append([0.011, 550, 0.0846])\n",
    "xdata.append([0.011, 750, 0.1154])\n",
    "xdata.append([0.011, 950, 0.1462])\n",
    "xdata.append([0.011, 1150, 0.177])\n",
    "xdata.append([0.011, 850, 0.1308])\n",
    "xdata.append([0.011, 550, 0.06047])\n",
    "xdata.append([0.011, 750, 0.08246])\n",
    "xdata.append([0.011, 950, 0.1044])\n",
    "xdata.append([0.011, 1050, 0.1134])\n",
    "xdata.append([0.011, 1150, 0.1264])\n",
    "xdata.append([0.011, 850, 0.0934])\n",
    "xdata.append([0.011, 550, 0.047])\n",
    "xdata.append([0.011, 750, 0.06413])\n",
    "xdata.append([0.011, 950, 0.08124])\n",
    "xdata.append([0.011, 1150, 0.09834])\n",
    "xdata.append([0.011, 850, 0.072691])\n",
    "xdata.append([0.011, 700, 0.087196])\n",
    "xdata.append([0.013, 550, 0.10005])\n",
    "xdata.append([0.013, 750, 0.13644])\n",
    "xdata.append([0.013, 950, 0.17282])\n",
    "xdata.append([0.013, 1150, 0.2092])\n",
    "xdata.append([0.013, 850, 0.15463])\n",
    "xdata.append([0.013, 550, 0.07147])\n",
    "xdata.append([0.013, 750, 0.09745])\n",
    "xdata.append([0.013, 950, 0.12344])\n",
    "xdata.append([0.013, 1050, 0.13302])\n",
    "xdata.append([0.013, 1150, 0.1494])\n",
    "xdata.append([0.013, 850, 0.11045])\n",
    "xdata.append([0.013, 550, 0.05558])\n",
    "xdata.append([0.013, 750, 0.0758])\n",
    "xdata.append([0.013, 950, 0.09601])\n",
    "xdata.append([0.013, 1150, 0.1162])\n",
    "xdata.append([0.013, 850, 0.0859])\n",
    "\n",
    "#ydata.append([ exit quality, max wall temperature (deg C)])\n",
    "ydata.append([0.525, 306.7])\n",
    "ydata.append([0.525, 298.5])\n",
    "ydata.append([0.525, 294.5])\n",
    "ydata.append([0.525, 290.2])\n",
    "ydata.append([0.524, 286.9])\n",
    "ydata.append([0.524, 284.1])\n",
    "ydata.append([0.525, 281.7])\n",
    "ydata.append([0.524, 290.3])\n",
    "ydata.append([0.734, 307.9])\n",
    "ydata.append([0.735, 295.5])\n",
    "ydata.append([0.735, 287.8])\n",
    "ydata.append([0.735, 285.0])\n",
    "ydata.append([0.735, 282.5])\n",
    "ydata.append([0.734, 291.3])\n",
    "ydata.append([ 0.945, 308.6])\n",
    "ydata.append([0.945, 296.2])\n",
    "ydata.append([0.945, 288.5])\n",
    "ydata.append([0.945, 283.1])\n",
    "ydata.append([0.945, 291.9])\n",
    "ydata.append([ 0.525, 328.0])\n",
    "ydata.append([0.525, 311.2])\n",
    "ydata.append([0.525, 300.8])\n",
    "ydata.append([0.525, 293.6])\n",
    "ydata.append([0.525, 305.5])\n",
    "ydata.append([0.735, 329.6])\n",
    "ydata.append([0.735, 312.6])\n",
    "ydata.append([0.735, 302.0])\n",
    "ydata.append([0.735, 299.4])\n",
    "ydata.append([0.735, 294.8])\n",
    "ydata.append([0.735, 306.8])\n",
    "ydata.append([ 0.945, 330.7])\n",
    "ydata.append([0.945, 313.6])\n",
    "ydata.append([0.944, 302.9])\n",
    "ydata.append([0.945, 295.6])\n",
    "ydata.append([0.944, 307.7])\n",
    "ydata.append([0.734, 324.7])\n",
    "ydata.append([0.525, 342.2])\n",
    "ydata.append([0.524, 322.3])\n",
    "ydata.append([0.524, 310.0])\n",
    "ydata.append([0.525, 301.6])\n",
    "ydata.append([0.524, 315.5])\n",
    "ydata.append([0.734, 344.1])\n",
    "ydata.append([0.735, 324.0])\n",
    "ydata.append([0.735, 311.5])\n",
    "ydata.append([0.735, 306.3])\n",
    "ydata.append([0.735, 302.9])\n",
    "ydata.append([0.734, 317.1])\n",
    "ydata.append([0.945, 345.3])\n",
    "ydata.append([0.944, 325.1])\n",
    "ydata.append([0.944, 312.5])\n",
    "ydata.append([0.945, 303.9])\n",
    "ydata.append([0.945, 318.2])\n",
    "xarray= np.array(xdata)\n",
    "yarray= np.array(ydata)\n",
    "print (xdata)\n",
    "print (xarray)\n",
    "print (ydata)\n",
    "print (yarray)\n",
    "\n",
    "#convert to:\n",
    "# median values of output variables\n",
    "ND=len(xarray)\n",
    "Taall = [row[0] for row in xdata]\n",
    "Tamed=statistics.median(Taall)\n",
    "IDall = [row[1] for row in xdata]\n",
    "IDmed=statistics.median(IDall)\n",
    "RLall = [row[2] for row in xdata]\n",
    "RLmed=statistics.median(RLall)\n",
    "\n",
    "VLall = [row[0] for row in ydata]\n",
    "VLmed=statistics.median(VLall)\n",
    "Wdall = [row[1] for row in ydata]\n",
    "Wdmed=statistics.median(Wdall)\n",
    "\n",
    "xdata = []\n",
    "ydata = []\n",
    "for i in range(ND):\n",
    "    xdata.append([xarray[i,0]/Tamed,xarray[i,1]/IDmed,xarray[i,2]/RLmed])\n",
    "    ydata.append([yarray[i,0]/VLmed,yarray[i,1]/Wdmed])\n",
    "\n",
    "xarray= np.array(xdata)\n",
    "yarray= np.array(ydata)\n",
    "\n",
    "#print (yarray)\n",
    "\n",
    "data_inputs = np.array(xdata)\n",
    "data_outputs = np.array(ydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "379a75b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.18181818 0.88235294 1.05180788]\n",
      " [0.72727273 1.11764706 1.14786832]\n",
      " [1.         0.88235294 1.24554776]\n",
      " [1.18181818 1.35294118 1.25418241]\n",
      " [0.72727273 0.64705882 0.66454398]\n",
      " [0.72727273 0.64705882 0.36913114]\n",
      " [1.18181818 1.         1.66896924]\n",
      " [1.         1.         1.00809498]\n",
      " [1.18181818 1.35294118 1.61252024]\n",
      " [0.72727273 1.35294118 1.38909876]\n",
      " [1.         1.35294118 1.36427415]\n",
      " [1.         1.         1.41176471]\n",
      " [1.18181818 0.64705882 0.59989207]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training_input,validation_input,training_output,validation_output = train_test_split(xarray,\n",
    "yarray, test_size=0.25, random_state=13)\n",
    "# print to check the shape of training and validation set\n",
    "training_input= np.array(training_input)#[:,[1,2,3]])\n",
    "training_output= np.array(training_output)\n",
    "validation_input= np.array(validation_input)#[:,[1,2,3]])\n",
    "validation_output= np.array(validation_output)\n",
    "\n",
    "print(validation_input)\n",
    "\n",
    "xarray=training_input\n",
    "yarray=training_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "60e227b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define neural network model\n",
    "#As seen below, we have created four dense layers.\n",
    "#A dense layer is a layer in neural network that’s fully connected.\n",
    "#In other words, all the neurons in one layer are connected to all other neurons in the next layer.\n",
    "#In the first layer, we need to provide the input shape, which is 1 in our case.\n",
    "#The activation function we have chosen is elu, which stands for exponential linear unit. .\n",
    "from keras import backend as K\n",
    "#initialize weights with values between -0.2 and 0.5\n",
    "initializer = keras.initializers.RandomUniform(minval= -0.2, maxval=0.5)\n",
    "model = keras.Sequential([\n",
    "keras.layers.Dense(6, activation=K.elu, input_shape=[3], kernel_initializer=initializer),\n",
    "keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(16, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(8, activation=K.elu, kernel_initializer=initializer),\n",
    "keras.layers.Dense(2, kernel_initializer=initializer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "357a03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We’re using RMSprop as our optimizer here. RMSprop stands for Root Mean Square Propagation.\n",
    "#It’s one of the most popular gradient descent optimization algorithms for deep learning networks.\n",
    "#RMSprop is an optimizer that’s reliable and fast.\n",
    "#We’re compiling the mode using the model.compile function. The loss function used here\n",
    "#is mean squared error. After the compilation of the model, we’ll use the fit method with ~500 epochs.\n",
    "#Number of epochs can be varied.\n",
    "#from tf.keras import optimizers\n",
    "rms = keras.optimizers.RMSprop(0.01)\n",
    "model.compile(loss='mean_absolute_error',optimizer=rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "07a87d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.1059\n",
      "Epoch 2/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 3/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0557\n",
      "Epoch 4/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0444\n",
      "Epoch 5/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0482\n",
      "Epoch 6/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0664\n",
      "Epoch 7/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0430\n",
      "Epoch 8/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0799\n",
      "Epoch 9/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0483\n",
      "Epoch 10/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0334\n",
      "Epoch 11/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0654\n",
      "Epoch 12/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0574\n",
      "Epoch 13/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0407\n",
      "Epoch 14/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0323\n",
      "Epoch 15/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0352\n",
      "Epoch 16/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0606\n",
      "Epoch 17/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0449\n",
      "Epoch 18/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0394\n",
      "Epoch 19/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0647\n",
      "Epoch 20/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0445\n",
      "Epoch 21/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0509\n",
      "Epoch 22/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0390\n",
      "Epoch 23/800\n",
      "2/2 [==============================] - 0s 807us/step - loss: 0.0619\n",
      "Epoch 24/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0420\n",
      "Epoch 25/800\n",
      "2/2 [==============================] - 0s 41us/step - loss: 0.0371\n",
      "Epoch 26/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0371\n",
      "Epoch 27/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0559\n",
      "Epoch 28/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0498\n",
      "Epoch 29/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0405\n",
      "Epoch 30/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0496\n",
      "Epoch 31/800\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0496\n",
      "Epoch 32/800\n",
      "2/2 [==============================] - 0s 787us/step - loss: 0.0357\n",
      "Epoch 33/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0474\n",
      "Epoch 34/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0429\n",
      "Epoch 35/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0414\n",
      "Epoch 36/800\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0407\n",
      "Epoch 37/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0379\n",
      "Epoch 38/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0530\n",
      "Epoch 39/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0413\n",
      "Epoch 40/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0374\n",
      "Epoch 41/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0418\n",
      "Epoch 42/800\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0473\n",
      "Epoch 43/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0443\n",
      "Epoch 44/800\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0325\n",
      "Epoch 45/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0401\n",
      "Epoch 46/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0570\n",
      "Epoch 47/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0424\n",
      "Epoch 48/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0586\n",
      "Epoch 49/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0301\n",
      "Epoch 50/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0376\n",
      "Epoch 51/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0488\n",
      "Epoch 52/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0362\n",
      "Epoch 53/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0354\n",
      "Epoch 54/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0434\n",
      "Epoch 55/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0329\n",
      "Epoch 56/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0326\n",
      "Epoch 57/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0633\n",
      "Epoch 58/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0429\n",
      "Epoch 59/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0404\n",
      "Epoch 60/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0414\n",
      "Epoch 61/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0532\n",
      "Epoch 62/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0394\n",
      "Epoch 63/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0645\n",
      "Epoch 64/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0481\n",
      "Epoch 65/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0251\n",
      "Epoch 66/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0522\n",
      "Epoch 67/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0270\n",
      "Epoch 68/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0584\n",
      "Epoch 69/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0280\n",
      "Epoch 70/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0395\n",
      "Epoch 71/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0599\n",
      "Epoch 72/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0759\n",
      "Epoch 73/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0321\n",
      "Epoch 74/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0432\n",
      "Epoch 75/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0354\n",
      "Epoch 76/800\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.0295\n",
      "Epoch 77/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0446\n",
      "Epoch 78/800\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0435\n",
      "Epoch 79/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0406\n",
      "Epoch 80/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0273\n",
      "Epoch 81/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0450\n",
      "Epoch 82/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0581\n",
      "Epoch 83/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0315\n",
      "Epoch 84/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0483\n",
      "Epoch 85/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0433\n",
      "Epoch 86/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0271\n",
      "Epoch 87/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0297\n",
      "Epoch 88/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0444\n",
      "Epoch 89/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0283\n",
      "Epoch 90/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0510\n",
      "Epoch 91/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0461\n",
      "Epoch 92/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0274\n",
      "Epoch 93/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0415\n",
      "Epoch 94/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0360\n",
      "Epoch 95/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0289\n",
      "Epoch 96/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0341\n",
      "Epoch 97/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0463\n",
      "Epoch 98/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0511\n",
      "Epoch 99/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0306\n",
      "Epoch 100/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0618\n",
      "Epoch 101/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0327\n",
      "Epoch 102/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0322\n",
      "Epoch 103/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0328\n",
      "Epoch 104/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0307\n",
      "Epoch 105/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0415\n",
      "Epoch 106/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0375\n",
      "Epoch 107/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0454\n",
      "Epoch 108/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 109/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0253\n",
      "Epoch 110/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0336\n",
      "Epoch 111/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0428\n",
      "Epoch 112/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0377\n",
      "Epoch 113/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0488\n",
      "Epoch 114/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0553\n",
      "Epoch 115/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0541\n",
      "Epoch 116/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0596\n",
      "Epoch 117/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0420\n",
      "Epoch 118/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0379\n",
      "Epoch 119/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 120/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0433\n",
      "Epoch 121/800\n",
      "2/2 [==============================] - 0s 332us/step - loss: 0.0237\n",
      "Epoch 122/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0332\n",
      "Epoch 123/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0313\n",
      "Epoch 124/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0430\n",
      "Epoch 125/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0625\n",
      "Epoch 126/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0387\n",
      "Epoch 127/800\n",
      "2/2 [==============================] - 0s 792us/step - loss: 0.0292\n",
      "Epoch 128/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0487\n",
      "Epoch 129/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0445\n",
      "Epoch 130/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0442\n",
      "Epoch 131/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0505\n",
      "Epoch 132/800\n",
      "2/2 [==============================] - 0s 536us/step - loss: 0.0378\n",
      "Epoch 133/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0332\n",
      "Epoch 134/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0387\n",
      "Epoch 135/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0436\n",
      "Epoch 136/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0368\n",
      "Epoch 137/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0420\n",
      "Epoch 138/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0506\n",
      "Epoch 139/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0386\n",
      "Epoch 140/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0424\n",
      "Epoch 141/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0431\n",
      "Epoch 142/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0411\n",
      "Epoch 143/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0250\n",
      "Epoch 144/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0434\n",
      "Epoch 145/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0379\n",
      "Epoch 146/800\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0477\n",
      "Epoch 147/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0311\n",
      "Epoch 148/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0503\n",
      "Epoch 149/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0275\n",
      "Epoch 150/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0490\n",
      "Epoch 151/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0498\n",
      "Epoch 152/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0262\n",
      "Epoch 153/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0307\n",
      "Epoch 154/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0331\n",
      "Epoch 155/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0649\n",
      "Epoch 156/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0316\n",
      "Epoch 157/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0369\n",
      "Epoch 158/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0391\n",
      "Epoch 159/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0420\n",
      "Epoch 160/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0542\n",
      "Epoch 161/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0475\n",
      "Epoch 162/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0406\n",
      "Epoch 163/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0258\n",
      "Epoch 164/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0485\n",
      "Epoch 165/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0503\n",
      "Epoch 166/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0420\n",
      "Epoch 167/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0412\n",
      "Epoch 168/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0310\n",
      "Epoch 169/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0221\n",
      "Epoch 170/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0301\n",
      "Epoch 171/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0656\n",
      "Epoch 172/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0399\n",
      "Epoch 173/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0480\n",
      "Epoch 174/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0374\n",
      "Epoch 175/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0490\n",
      "Epoch 176/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0407\n",
      "Epoch 177/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0494\n",
      "Epoch 178/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0381\n",
      "Epoch 179/800\n",
      "2/2 [==============================] - 0s 932us/step - loss: 0.0299\n",
      "Epoch 180/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0273\n",
      "Epoch 181/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0496\n",
      "Epoch 182/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0446\n",
      "Epoch 183/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0293\n",
      "Epoch 184/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0406\n",
      "Epoch 185/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0468\n",
      "Epoch 186/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0359\n",
      "Epoch 187/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0322\n",
      "Epoch 188/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0429\n",
      "Epoch 189/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0520\n",
      "Epoch 190/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0312\n",
      "Epoch 191/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0292\n",
      "Epoch 192/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0279\n",
      "Epoch 193/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0496\n",
      "Epoch 194/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0466\n",
      "Epoch 195/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0336\n",
      "Epoch 196/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0356\n",
      "Epoch 197/800\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.0316\n",
      "Epoch 198/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0313\n",
      "Epoch 199/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0383\n",
      "Epoch 200/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0448\n",
      "Epoch 201/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0367\n",
      "Epoch 202/800\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0316\n",
      "Epoch 203/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0644\n",
      "Epoch 204/800\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.0489\n",
      "Epoch 205/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0523\n",
      "Epoch 206/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0399\n",
      "Epoch 207/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0381\n",
      "Epoch 208/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0562\n",
      "Epoch 209/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0488\n",
      "Epoch 210/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0371\n",
      "Epoch 211/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0343\n",
      "Epoch 212/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0593\n",
      "Epoch 213/800\n",
      "2/2 [==============================] - 0s 525us/step - loss: 0.0247\n",
      "Epoch 214/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0536\n",
      "Epoch 215/800\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.0423\n",
      "Epoch 216/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0343\n",
      "Epoch 217/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0286\n",
      "Epoch 218/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0396\n",
      "Epoch 219/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0564\n",
      "Epoch 220/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0474\n",
      "Epoch 221/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0238\n",
      "Epoch 222/800\n",
      "2/2 [==============================] - 0s 807us/step - loss: 0.0592\n",
      "Epoch 223/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0284\n",
      "Epoch 224/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0492\n",
      "Epoch 225/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0405\n",
      "Epoch 226/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0414\n",
      "Epoch 227/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0479\n",
      "Epoch 228/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0411\n",
      "Epoch 229/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0335\n",
      "Epoch 230/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0343\n",
      "Epoch 231/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0330\n",
      "Epoch 232/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0598\n",
      "Epoch 233/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0541\n",
      "Epoch 234/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0417\n",
      "Epoch 235/800\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.0321\n",
      "Epoch 236/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0529\n",
      "Epoch 237/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0303\n",
      "Epoch 238/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0506\n",
      "Epoch 239/800\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0481\n",
      "Epoch 240/800\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.0264\n",
      "Epoch 241/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0477\n",
      "Epoch 242/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0433\n",
      "Epoch 243/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0530\n",
      "Epoch 244/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0294\n",
      "Epoch 245/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0593\n",
      "Epoch 246/800\n",
      "2/2 [==============================] - 0s 0s/step - loss: 0.0493\n",
      "Epoch 247/800\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.0440\n",
      "Epoch 248/800\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.0236\n",
      "Epoch 249/800\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0438Restoring model weights from the end of the best epoch.\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.0404\n",
      "Epoch 00249: early stopping\n",
      "best epoch =  169\n",
      "smallest loss = 0.022120485082268715\n",
      "INFO:tensorflow:Assets written to: ./best_model\\assets\n"
     ]
    }
   ],
   "source": [
    "#After the compilation of the model, we’ll use the fit method with 500 epochs.\n",
    "#I started with epochs value of 100 and then tested the model after training.\n",
    "#The prediction was not that good. Then I modified the number of epochs to 200 and tested the model again.\n",
    "#Accuracy had improved slightly, but figured I’d give it one more try. Finally, at 500 epochs\n",
    "#I found acceptable prediction accuracy.\n",
    "#The fit method takes three parameters; namely, x, y, and number of epochs.\n",
    "#During model training, if all the batches of data are seen by the model once,\n",
    "#we say that one epoch has been completed.\n",
    "# Add an early stopping callback\n",
    "es = keras.callbacks.EarlyStopping(\n",
    "monitor='loss',\n",
    "mode='min',\n",
    "patience = 80,\n",
    "restore_best_weights = True,\n",
    "verbose=1)\n",
    "# Add a checkpoint where loss is minimum, and save that model\n",
    "mc = keras.callbacks.ModelCheckpoint('best_model.SB', monitor='loss',\n",
    "mode='min', verbose=1, save_best_only=True)\n",
    "historyData = model.fit(xarray,yarray,epochs=800,callbacks=[es])\n",
    "loss_hist = historyData.history['loss']\n",
    "#The above line will return a dictionary, access it's info like this:\n",
    "best_epoch = np.argmin(historyData.history['loss']) + 1\n",
    "print ('best epoch = ', best_epoch)\n",
    "print('smallest loss =', np.min(loss_hist))\n",
    "model.save('./best_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e782636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line of code can be used to reconstruct the saved model.\n",
    "recon_model = keras.models.load_model(\"best_model\")\n",
    "# the name of the model is now \"recon_model\". You can then use this model to do predictions for comparisons.\n",
    "# See the previous project for code to do the comparisons.\n",
    "# NOTE: If you get an error message when trying to run with the line\n",
    "# recon_model = tf.keras.models.load_model(\"best_model\")\n",
    "# try running prediction calculations with model.predict() (as in the code for project 2)\n",
    "# with the line recon_model = tf.keras.models.load_model(\"best_model\") removed.\n",
    "# That may avoid the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d309a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
